{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "Based on actions, agents get rewards or -ve rewards. It learns from it's experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "class SampleEnvironment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 20\n",
    "        \n",
    "    def get_observations(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return [0, 1]\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def action(self, action: int) -> float:\n",
    "        if(self.is_done()):\n",
    "            raise Exception(\"Game Over! You Won!\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def step(self, env: SampleEnvironment):\n",
    "        current_obs = env.get_observations()\n",
    "        print(\"current_obs {} \".format(current_obs))\n",
    "        \n",
    "        actions = env.get_actions()\n",
    "        print(actions)\n",
    "        \n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward     \n",
    "        print(\"Total reward {}\".format(self.total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 0.14108054880388732\n",
      "Steps: 2\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 0.793083609340115\n",
      "Steps: 3\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 1.6198167763815312\n",
      "Steps: 4\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 1.7711422456822543\n",
      "Steps: 5\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 2.700783994175965\n",
      "Steps: 6\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 2.9593270069073805\n",
      "Steps: 7\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 3.653495734956566\n",
      "Steps: 8\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 4.331816211860455\n",
      "Steps: 9\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 4.388724447603204\n",
      "Steps: 10\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 4.78310325433358\n",
      "Steps: 11\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 5.626044709285944\n",
      "Steps: 12\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 5.917054685118135\n",
      "Steps: 13\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 6.506577467408446\n",
      "Steps: 14\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 7.292159888763235\n",
      "Steps: 15\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 7.375488051710075\n",
      "Steps: 16\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 8.070421554675804\n",
      "Steps: 17\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 8.691927846581587\n",
      "Steps: 18\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 9.283656897762654\n",
      "Steps: 19\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 9.864139010319404\n",
      "Steps: 20\n",
      "current_obs [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total reward 10.695243749654281\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = SampleEnvironment()\n",
    "    agent = Agent()\n",
    "    \n",
    "    i = 0\n",
    "    while not env.is_done():\n",
    "        i += 1\n",
    "        print(\"Steps: {}\".format(i))\n",
    "        agent.step(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
